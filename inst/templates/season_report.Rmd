---
title: "Harvest Information Program Season Summary Report"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
    theme: cerulean
params:
  comp_path:
    value: x
  proc_path:
    value: x
  year:
    value: x
---

```{r libs, include = FALSE}
library(tidyverse)
library(DT)
library(shiny)
library(stringi)
```

```{r import, include = FALSE}

# Get the directories for the season
dirs <- 
  list.files(params$comp_path, recursive = TRUE)  %>%
  as_tibble() %>%
  mutate(
    value = 
      str_extract(value, "DL[0-9]{4}") %>% 
      paste0(params$comp_path, .)) %>% 
  distinct()

# Get total number of lines of data
season_lines <- map_dbl(1:nrow(dirs), ~sumLines(dirs[.x,] %>% pull()))
rm(dirs)

# Run the read function
dl_data <- read_hip(params$comp_path, season = TRUE) 

# Standardize birth date format
fixed_dates_data <-
  dl_data %>% 
  mutate(
    birth_date2 =
      ifelse(
        str_detect(birth_date, "^[0-9]{1}\\/"),
        paste0("0", birth_date),
        birth_date),
    birth_date3 =
      ifelse(
        str_length(birth_date2) == 9,
        paste0(
          str_sub(birth_date2, 1, 3),
          "0",
          str_sub(birth_date2, 4, 9)),
        birth_date2)) %>% 
  select(-c("birth_date", "birth_date2")) %>%
  rename(birth_date = birth_date3) %>%
  relocate(birth_date, .before = "issue_date") 

# Snoop for line shift errors
error_keys <- 
  fixed_dates_data %>% 
  filter(
    !str_detect(
      birth_date, "^[0-9]{2}\\/[0-9]{2}\\/[0-9]{4}$")) %>% 
  relocate(record_key, .before = "title") %>% 
  relocate(source_file, .before = "record_key") %>%
  select(record_key) %>% 
  pull()

# Correct frame shifts for x number of records
unshifted <-
  map_dfr(
    1:length(error_keys),
    ~shiftFix(fixed_dates_data, error_keys[.x], "state", "email", 1)
  ) %>% 
  # Delete the 0s from end of zip
  mutate(zip = str_remove(zip, "0$")) %>%
  mutate(
    birth_date =
      ifelse(
        str_detect(birth_date, "^[0-9]{1}\\/"),
        paste0("0", birth_date),
        birth_date))

rm(error_keys)

unshifted_data <-
  fixed_dates_data %>%
  filter(!record_key %in% unshifted$record_key) %>%
  bind_rows(unshifted) 

rm(fixed_dates_data)
rm(unshifted)

# Run the clean function
cleaned_data <- clean(unshifted_data)

rm(unshifted_data)

# Run the fixDuplicates function
fixed_data <- fixDuplicates(cleaned_data)

# Run the proof function
proofed_data <- proof(fixed_data, year = params$year)

rm(fixed_data)

# Read in pre-processed data
corrected_data <-
  map_df(1:length(list.files(params$proc_path)),
      ~read_csv(list.files(params$proc_path, full.names = T)[.x],
                col_types = paste(rep("c", 28), collapse = "")) %>% 
        mutate(dl_cycle = str_extract(list.files(params$proc_path)[.x], "(?<=DL)[0-9]{4}"))) %>% 
  rename(
    zip = postal_code,
    ducks_bag = Q_ducks,
    geese_bag = Q_geese,
    dove_bag = Q_doves,
    woodcock_bag = Q_woodcock,
    coots_snipe = Q_coot_snipe,
    rails_gallinules = Q_rail_gallinule,
    cranes = Q_cranes,
    band_tailed_pigeon = Q_bt_pigeons,
    brant = Q_brant,
    seaducks = Q_seaducks) %>% 
  mutate(record_key = paste0("record_", row_number())) %>% 
  relocate(record_key, .after = "source_file")

# Summarize number of records from Canada
canada_records <-
  proofed_data %>% 
  filter(
    state %in% c("AB", "BC", "MB", "NB", "NL", "NT", "NS", 
                 "NU", "ON", "PE", "QC", "SK", "YT"))

```

# Introduction

This is a summary report of the `r params$year`-`r params$year+1` season of Harvest Information Program (HIP) data. This year, `r sum(season_lines) %>% as.character()` total records and `r nrow(dl_data)` unique records were submitted in `r length(list.files(params$comp_path, recursive = TRUE) %>% as_tibble() %>% transmute(filepath = as.character(value)) %>% mutate(filepath = str_replace(filepath, "TXT", "txt")) %>% filter(str_detect(filepath, "(?<=\\.)txt$")) %>% filter(!str_detect(filepath, "permit")) %>% filter(!str_detect(filepath, "removed")) %>% pull())` files from `r proofed_data %>% select(dl_state) %>% distinct %>% nrow()` states. A total of `r nrow(canada_records)` registered hunters were from Canada.

Below is a table summarizing the total of number of records per download state in the corrected data.

```{r sum_per_state, echo = FALSE, message = FALSE}

DT::datatable(
  corrected_data %>% 
    select(dl_state) %>% 
    group_by(dl_state) %>% 
    summarize(sum = n()) %>% 
    ungroup() %>% 
    arrange(desc(sum)))

rm(canada_records)
rm(season_lines)
```

# Data Checking

## Registration year checking

Below is a table summarizing the values for registration year in the corrected data.

```{r reg_yr_check, echo = FALSE}
DT::datatable(
  filter = "top", 
  corrected_data %>% 
    select(registration_yr, dl_state) %>% 
    group_by(registration_yr, dl_state) %>% 
    summarize(n = n()) %>% 
    ungroup() %>% 
    arrange(desc(n)))

```

## HuntY response checking

Below is a table summarizing the total of number of hunt_mig_birds responses per download state in the corrected data.

```{r hunty_check, echo = FALSE}
huntcheck <-
  corrected_data %>% 
  select(hunt_mig_birds, dl_state) %>% 
  group_by(hunt_mig_birds, dl_state) %>% 
  summarize(n = n()) %>% 
  ungroup() 

DT::datatable(huntcheck)

rm(huntcheck)
```

## PII checking

The following records are missing first name, last name, date of birth, address, state, and/or city AND zip. They are not included in the final output table.

```{r record_check, echo = FALSE, fig.align = "center"}
record_check <- 
  dl_data %>%
  # Filter out records if firstname, lastname, city of residence, state of
  # residence, or date of birth are missing -- records discarded because
  # these are needed to identify individuals
  # Discard additional records if they are missing elements of an address
  # and we have no way to resolve it from remaining information (i.e. state
  # from zip)
  filter(
    is.na(firstname)|
      is.na(lastname)|
      is.na(state)|
      is.na(birth_date)|
      is.na(address)|
      (is.na(city)&is.na(zip)))

if(nrow(record_check) > 0){
  missing_plot <-
    record_check %>%
    select(firstname, lastname, birth_date, state, address, city, zip) %>%
    # Add an ID per row
    mutate(hunter_id = row_number()) %>%
    # Pivot the field names to long format
    pivot_longer(firstname:zip, names_to = "field") %>%
    # Only keep hunters' fields with NA values
    filter(is.na(value)) %>%
    # Set NA to 1 for plotting
    mutate(value = 1) %>%
    # Make a heat map
    ggplot(aes(x = field, y = as.factor(hunter_id), fill = value)) +
    geom_tile() +
    labs(y = "Hunter ID", x = "Data Field") +
    theme_classic() +
    theme(legend.position = "none",
          axis.text.y = element_blank(),
          axis.ticks.y = element_blank())

  print(missing_plot)}else{
  message("No records detected with missing PII.")
}
```

There are `r nrow(record_check)` total records with missing PII. Below is a summary of the number of records missing from each HIP file.

```{r pii_by_file, echo = FALSE}
pii_summary <- 
  record_check %>% 
  group_by(source_file) %>% 
  summarize(n = n()) %>% 
  ungroup() %>% 
  left_join(
    dl_data %>% 
      select(source_file) %>% 
      group_by(source_file) %>% 
      mutate(total_rows = n()) %>% 
      ungroup(),
    by = "source_file") %>% 
  mutate(prop = round(n/total_rows, digits = 2)) %>% 
  select(-total_rows) %>% 
  distinct() %>% 
  arrange(desc(prop)) 

DT::datatable(pii_summary)
rm(pii_summary)
rm(dl_data)
```

## Missing email addresses

Below is a figure showing the total of number of missing or incorrectly formatted email addresses in the corrected data.

```{r missing_emails, echo = FALSE, warning = FALSE, fig.align = "center"}

corrected_data_proofed <-
  proof(corrected_data, year = params$year)

corrected_data_proofed %>% 
  select(dl_state, email, errors) %>% 
  group_by(dl_state) %>% 
  mutate(total_n = n()) %>% 
  ungroup() %>% 
  filter(is.na(email)|errors == "email") %>% 
  group_by(dl_state, total_n) %>% 
  summarize(missing_email_n = n()) %>% 
  ungroup() %>% 
  mutate(prop = missing_email_n/total_n) %>% 
  arrange(desc(prop)) %>% 
  # Plot
  ggplot() +
  geom_bar(aes(
    y = prop,
    x = reorder(dl_state, prop)),
    stat = "identity") +
  geom_text(
    aes(
      y = prop,
      x = reorder(dl_state, prop),
      label = missing_email_n,
      angle = 90),
    vjust = 0.2, hjust = -0.2) +
  labs(
    x = "State",
    y = "Missing email\naddresses (proportion)") +
  #scale_y_continuous() +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + 
  scale_y_continuous(
    breaks = seq(0, 1, 0.25), expand = expansion(mult = c(-0, 0.3)))
```

## Duplicates

### Before fixDuplicates (all 49 states)

Below is a figure showing the total of number of partial duplicates in the cleaned data. Categories on the x-axis bin partial duplicates based on what field(s) were inconsistent among duplicates.

```{r b_fd, echo = FALSE, warning = FALSE, fig.align = "center"}
findDuplicates(cleaned_data)
```

### After fixDuplicates (all 49 states)

Below is a figure showing the total of number of partial duplicates in the corrected data. These duplicates are among downloads. Records that are duplicated with the only difference being the "dl_cycle" field are exact duplicates; the same record was sent more than once by a state.

```{r a_fd, echo = FALSE, warning = FALSE, fig.align = "center"}
rm(cleaned_data)
findDuplicates(corrected_data)
```

## Strata

Below is a table summarizing the bag values ("state_strata") per download state and species that fall outside our expected set of strata values ("normal_strata") in the corrected data.

```{r stratacheck, echo = FALSE}
DT::datatable(strataCheck(corrected_data))
```

## Repeated values

### Repeated values across duck, goose, and snipe_coot bag codes

Below is a table summarizing the number of records per file that have the same value repeated in the duck, goose, and snipe_coot fields in the corrected data. The "h_value" column contains the repeated value, "h_rep" indicates the number of rows that had repeated values in all four fields, "h_total" indicates the number of total rows in the file, and "prop_repeat" is the proportion of rows with repetition in the file.

```{r h_validate, echo = FALSE}
DT::datatable(
  migbirdHIP::validate(corrected_data, type = "horizontal") %>% 
    mutate(prop_repeat = round(prop_repeat, digits = 2)))
```

### Repeated values within duck bag code

The table below summarizes corrected data by state and download cycle to show the number of vertically repeated values ("v_repeated") in ducks_bag. To find out what the vertically repeated values were, please refer to the corresponding download report.

```{r v_validate, echo = FALSE}
DT::datatable(migbirdHIP::validate(corrected_data, type = "vertical", period = "dl_cycle") %>% arrange(desc(v_repeated)))
```

## Zero values

### Potentially not HIP registrations

The table below summarizes the corrected data to indicate the number of rows per file with 0s in <b>all of the bag fields</b>. The "total_rows" column is the number of total rows in the source file, "all_zero_rows" is the number of rows that had all 0s, and "zero_proportion" is the proportion of all-zero rows per file.

```{r zero_all, echo = FALSE, warning = FALSE}

DT::datatable(
  corrected_data %>%
    # Subset the data
    select(
      dl_state,
      source_file,
      matches("bag|coots|rails|cranes|pigeon|brant|seaducks")) %>%
    group_by(source_file) %>%
    mutate(total_rows = n()) %>% 
    ungroup() %>% 
    relocate(total_rows, .before = "ducks_bag") %>% 
    group_by(dl_state, source_file, total_rows) %>% 
    mutate_at(
      vars(
        corrected_data %>% 
        select(matches("bag|coots|rails|cranes|pigeon|brant|seaducks")) %>% 
        names()), 
      ~suppressWarnings(as.integer(.))) %>% 
    ungroup() %>% 
    mutate(
      sumbags = 
          rowSums(
            select(
              cur_data(), 
              matches("bag|coots|rails|cranes|pigeon|brant|seaducks")))) %>% 
    filter(sumbags == 0) %>% 
    select(-matches("bag|coots|rails|cranes|pigeon|brant|seaducks")) %>% 
    group_by(dl_state, source_file, total_rows) %>% 
    mutate(all_zero_rows = n()) %>% 
    ungroup() %>% 
    distinct() %>% 
    mutate(zero_proportion = round(all_zero_rows / total_rows, 2)) %>% 
    arrange(desc(zero_proportion))
)

```

The table below summarizes the corrected data to indicate the number of rows per file with 0s in the <b>ducks_bag field</b>. The "total_rows" column is the number of total rows in the source file, "zero_rows" is the number of rows that had a 0 in the ducks_bag column, and "zero_ducks_prop" is the proportion of ducks_bag rows with a 0 per file.

```{r zero_ducks, echo = FALSE}
DT::datatable(
  corrected_data %>%
  # Subset the data
  select(dl_state, dl_date, ducks_bag) %>%
  group_by(dl_state, dl_date) %>%
  # Add number of rows per group
  # This is the number of records per file
  mutate(total_rows = n()) %>%
  # Move total_rows to 3rd col position
  relocate(total_rows, .after = dl_date) %>%
  ungroup() %>%
  # Filter to only ducks = 0
  filter(ducks_bag == 0) %>% 
  group_by(dl_state, dl_date, total_rows) %>%
  # Count the number of records per file with ducks = 0
  mutate(zero_rows = n()) %>%
  ungroup() %>%
  select(-ducks_bag) %>% 
  distinct() %>% 
  mutate(zero_ducks_prop = round(zero_rows / total_rows, 2)) %>% 
  arrange(desc(zero_ducks_prop)) %>% 
  relocate(zero_rows, .after = "total_rows")
)

```

# Error Visualizations

## By field

### Before correction

```{r erpf, echo = FALSE, fig.align = "center"}
errorPlot_fields(proofed_data, year = params$year)
```

### After correction

```{r erpf_c, echo = FALSE, fig.align = "center", warning = FALSE}

# Get the order of the fields so that they match the plot above
x_order <-
  proofed_data %>%
  select(errors) %>%
  # Pull errors apart, delimited by hyphens
  separate(errors, into = as.character(c(1:25)), sep = "-") %>%
  # Transform errors into a single column
  pivot_longer(1:25, names_to = "name") %>%
  select(errors = value) %>%
  filter(!is.na(errors)) %>%
  group_by(errors) %>%
  # Count number of correct values
  summarize(count_errors = sum(!is.na(errors))) %>%
  ungroup() %>%
  # Calculate error proportion
  mutate(
    total = nrow(proofed_data),
    proportion = count_errors / nrow(proofed_data)) %>%
  arrange(proportion)

left_join(
  x_order %>% 
    mutate(order = row_number()) %>% 
    select(errors, order), 
  corrected_data_proofed %>%
    select(errors) %>%
    # Pull errors apart, delimited by hyphens
    separate(errors, into = as.character(c(1:25)), sep = "-") %>%
    # Transform errors into a single column
    pivot_longer(1:25, names_to = "name") %>%
    select(errors = value) %>%
    filter(!is.na(errors)) %>%
    group_by(errors) %>%
    # Count number of correct values
    summarize(count_errors = sum(!is.na(errors))) %>%
    ungroup(),
  by = "errors") %>%
  mutate(count_errors = ifelse(is.na(count_errors), 0, count_errors)) %>% 
  # Calculate error proportion
  mutate(
    total = nrow(corrected_data),
    proportion = count_errors / nrow(corrected_data)) %>%
  # Plot
  ggplot() +
  geom_bar(
    aes(x = reorder(errors, order), y = proportion),
    stat = "identity") +
  geom_text(
    aes(x = errors, y = proportion, label = count_errors, angle = 90),
    vjust = 0.2, hjust = -0.2) +
  labs(
    x = "Field",
    y = "Error proportion",
    title = "Error proportion per field") +
  scale_y_continuous(expand = expansion(mult = c(-0, 0.25))) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

rm(corrected_data)
```

## By state

### Before correction

```{r erps, echo = FALSE, fig.align = "center"}
errorPlot_states(proofed_data)
```

### After correction

```{r erps_c, echo = FALSE, fig.align = "center"}
errorPlot_states(corrected_data_proofed)
```

## By download

### Before correction

```{r erpdl, echo = FALSE, fig.align = "center"}
errorPlot_dl(proofed_data)
```

### After correction

```{r erpdl_c, echo = FALSE, fig.align = "center"}
errorPlot_dl(corrected_data_proofed)

rm(corrected_data_proofed)
```

## Out-of-state hunters

```{r canadians, echo = FALSE, fig.align = "center"}
outOfStateHunters(proofed_data)
```

```{r canadians_tbl, echo = FALSE}
DT::datatable(
  outOfStateHunters(proofed_data, output = "table") %>% 
    arrange(desc(outofstate_prop)))
```

## Youth hunters

```{r yh, echo = FALSE, fig.align = "center"}
youthHunters(proofed_data, year = params$year)
```

# Error Exploration

A closer look at common errors and issues within this download's HIP data.

## High error counts

Q: What were the most common fields with errors, and what values caused them?

Most common error fields...

```{r et, echo = FALSE}
et <- errorTable(proofed_data, loc = "none") %>% arrange(desc(error_count))

DT::datatable(et)
```

Values associated with error in `r et$error[1]`

```{r pe_1, echo = FALSE, warning = FALSE}

if(et$error[1] != "birth_date"){
  err1 <- 
    pullErrors(proofed_data, error = et$error[1], distinct = FALSE) %>% 
    as_tibble() %>% 
    mutate(
      value = 
        suppressWarnings(
          stri_enc_toutf8(value, is_unknown_8bit = T, validate = T))) %>%
    group_by(value) %>% 
    summarize(count = n()) %>% 
    ungroup() %>% 
    arrange(desc(count))
  }else{
  err1 <- 
    pullErrors(proofed_data, error = et$error[1], distinct = FALSE) %>% 
    as_tibble() %>% 
    mutate(value = str_extract(value, ".{4}$")) %>% 
    group_by(value) %>% 
    summarize(count = n()) %>% 
    ungroup() %>% 
    arrange(desc(count))
}

if(et$error[1] == "email"){
  DT::datatable(
    err1 %>% 
      filter(count > 3))
}else{
  DT::datatable(err1)
}

rm(err1)
```

Values associated with error in `r et$error[2]`

```{r pe_2, echo = FALSE, warning = FALSE}

if(et$error[2] != "birth_date"){
  err2 <- 
    pullErrors(proofed_data, error = et$error[2], distinct = FALSE) %>% 
    as_tibble() %>% 
    mutate(
      value = 
        suppressWarnings(
          stri_enc_toutf8(value, is_unknown_8bit = T, validate = T))) %>%
    group_by(value) %>% 
    summarize(count = n()) %>% 
    ungroup() %>% 
    arrange(desc(count))
  }else{
  err2 <- 
    pullErrors(proofed_data, error = et$error[2], distinct = FALSE) %>% 
    as_tibble() %>% 
    mutate(value = str_extract(value, ".{4}$")) %>% 
    group_by(value) %>% 
    summarize(count = n()) %>% 
    ungroup() %>% 
    arrange(desc(count))
}

if(et$error[2] == "email"){
  DT::datatable(
    err2 %>% 
      filter(count > 3))
}else{
  DT::datatable(err2)
}

rm(err2)
```

Values associated with error in `r et$error[3]`

```{r pe_3, echo = FALSE, warning = FALSE}

if(et$error[3] != "birth_date"){
  err3 <- 
    pullErrors(proofed_data, error = et$error[3], distinct = FALSE) %>% 
    as_tibble() %>% 
    mutate(
      value = 
        suppressWarnings(
          stri_enc_toutf8(value, is_unknown_8bit = T, validate = T))) %>%
    group_by(value) %>% 
    summarize(count = n()) %>% 
    ungroup() %>% 
    arrange(desc(count))
  }else{
  err3 <- 
    pullErrors(proofed_data, error = et$error[3], distinct = FALSE) %>% 
    as_tibble() %>%  
    mutate(value = str_extract(value, ".{4}$")) %>% 
    group_by(value) %>% 
    summarize(count = n()) %>% 
    ungroup() %>% 
    arrange(desc(count))
}

if(et$error[3] == "email"){
  DT::datatable(
    err3 %>% 
      filter(count > 3))
}else{
  DT::datatable(err3)
}

rm(err3)
```

## High error proportions

Q: What states and fields had a proportion of error that exceeded an acceptable threshold (before correction)?

States with more than 1% error

```{r rfs, echo = FALSE}
redFlags(proofed_data, type = "state", threshold = 0.1)
```

Fields with more than 1% error

```{r rff, echo = FALSE}
redFlags(proofed_data, type = "field", threshold = 0.1)
rm(proofed_data)
```
