---
title: "Harvest Information Program Season Summary Report"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
    theme: cerulean
params:
  comp_path:
    value: x
  year:
    value: x
---

```{r libs, include = FALSE}
library(tidyverse)
library(DT)
library(shiny)
```

```{r import, include = FALSE}

season_data <- read_hip(params$comp_path)

cleaned_data <- clean(season_data)

# Remove original data to improve memory use
rm(season_data)

fixed_data <- fixDuplicates(cleaned_data)
fxd <- fixed_data$fixed_duplicates %>% as_tibble()
rm(fixed_data)

proofed_data <- proof(fxd, year = params$year)

corrected_data <- correct(proofed_data, year = params$year)

```

# Introduction

This is a summary report of the `r params$year` Season of Harvest Information Program (HIP) data. This year, `r nrow(proofed_data)` records were submitted from `r proofed_data %>% select(dl_state) %>% distinct %>% nrow()` states.

Below is a table summarizing the total of number of records per download state.

```{r sum_per_state, echo = FALSE, message = FALSE}

DT::datatable(cleaned_data %>% 
                select(dl_state) %>% 
                group_by(dl_state) %>% 
                summarize(sum = n()) %>% 
                ungroup())

```

# Data Checking

## Unique issue dates

```{r issuedates, echo = FALSE}
DT::datatable(cleaned_data %>% 
                select(issue_date, dl_state) %>% 
                distinct())
```

## Duplicates

### Before fixDuplicates (all 49 states)

```{r b_fd, echo = FALSE}
findDuplicates(cleaned_data)
```

### Before fixDuplicates (only permit states)

Permit states include AK, AZ, CO, KS, MN, MT, ND, NM, OK, OR, SD, TX, UT, WA, and WY.

```{r b_perms, echo = FALSE}
perm_states <- 
  c("AK", "AZ", "CO", "KS", "MN", "MT", "ND", "NM", "OK", "OR", "SD", "TX", 
    "UT", "WA", "WY")

findDuplicates(cleaned_data %>% filter(dl_state %in% perm_states))
```

### Before fixDuplicates (pick a state)

```{r b_fd_s, echo = FALSE}
acceptable_49_dl_states <-
      c("AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA", "ID", "IL",
        "IN", "IA", "KS", "KY", "LA", "ME", "MD", "MA", "MI", "MN", "MS", "MO",
        "MT", "NE", "NV", "NH", "NJ", "NM", "NY", "NC", "ND", "OH", "OK", "OR",
        "PA", "RI", "SC", "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI",
        "WY")

shinyApp(

  ui = fluidPage(
    selectInput("dl_state", "State:",
                choices = acceptable_49_dl_states),
    plotOutput("findDuplicatesPlot")
  ),

  server = function(input, output) {
    output$findDuplicatesPlot = renderPlot({
      
      cleaned_data %>%
        # Filter to input state
        filter(dl_state == input$dl_state) %>% 
        # Create a row key
        mutate(hunter_key = paste0("hunter_", row_number())) %>%
        # Group by registrant information; name, city, state, birthday, dl_state
        group_by(
          firstname,
          lastname,
          city,
          state,
          birth_date,
          dl_state) %>%
        # Identify duplicates
        mutate(
          duplicate =
            ifelse(
              n() > 1,
              "duplicate",
              "1")) %>%
        ungroup() %>%
        # Filter out non-duplicate records
        filter(duplicate == "duplicate") %>%
        # Sort tibble
        arrange(
          firstname,
          lastname,
          city,
          state,
          birth_date,
          dl_state) %>%
        select(-c("hunter_key", "duplicate")) %>%
        group_by(firstname, lastname, city, state, birth_date, dl_state) %>%
        mutate(
          # Hunter key per individual (not per row)
          hunter_key = cur_group_id(),
          # Find the reason for the duplicates
          # We start with a blank string so the following code can paste in
          dupl = "",
          # Iterate over each field in order to paste the field names together
          # (can't be done with case_when)
          dupl =
            ifelse(
              length(unique(title)) > 1,
              paste(dupl, "title", sep = "-"),
              dupl),
          dupl =
            ifelse(
              length(unique(middle)) > 1,
              paste(dupl, "middle", sep = "-"),
              dupl),
          dupl =
            ifelse(
              length(unique(suffix)) > 1,
              paste(dupl, "suffix", sep = "-"),
              dupl),
          dupl =
            ifelse(
              length(unique(address)) > 1,
              paste(dupl, "address", sep = "-"),
              dupl),
          dupl =
            ifelse(
              length(unique(zip)) > 1,
              paste(dupl, "zip", sep = "-"),
              dupl),
          dupl =
            ifelse(
              length(unique(birth_date)) > 1,
              paste(dupl, "birth_date", sep = "-"),
              dupl),
          dupl =
            ifelse(
              length(unique(issue_date)) > 1,
              paste(dupl, "issue_date", sep = "-"),
              dupl),
          dupl =
            ifelse(
              length(unique(hunt_mig_birds)) > 1,
              paste(dupl, "hunt_mig_birds", sep = "-"),
              dupl),
          dupl =
            ifelse(
              length(unique(registration_yr)) > 1,
              paste(dupl, "registration_yr", sep = "-"),
              dupl),
          dupl =
            ifelse(
              length(unique(email)) > 1,
              paste(dupl, "email", sep = "-"),
              dupl),
          dupl =
            ifelse(
              length(unique(dl_date)) > 1,
              paste(dupl, "dl_date", sep = "-"),
              dupl),
          dupl =
            ifelse(
              length(unique(dl_cycle)) > 1,
              paste(dupl, "dl_cycle", sep = "-"),
              dupl),
          dupl = ifelse(str_detect(dupl, "^$"), "bag", dupl),
          dupl = str_remove(dupl, "^\\-")
        ) %>%
        ungroup() %>%
        select(hunter_key, dupl) %>%
        distinct() %>%
          # Bin into generic "2+ fields" if more than one field contributes to a
          # duplicate
          mutate(
            dupl =
              case_when(
                str_detect(dupl, "[a-z|a-z\\_a-z|a-z|a-z\\_a-z\\_a-z|a-z\\_a-z]{1,}\\-[a-z|a-z\\_a-z]{1,}\\-[a-z|a-z\\_a-z]{1,}\\-[a-z|a-z\\_a-z]{1,}\\-[a-z|a-z\\_a-z]{1,}") ~ "2+ fields", #5+ fields
                str_detect(dupl, "[a-z|a-z\\_a-z]{1,}\\-[a-z|a-z\\_a-z]{1,}\\-[a-z|a-z\\_a-z]{1,}\\-[a-z|a-z\\_a-z]{1,}") ~ "2+ fields", #4 fields
                str_detect(dupl, "[a-z|a-z\\_a-z]{1,}\\-[a-z|a-z\\_a-z]{1,}\\-[a-z|a-z\\_a-z]{1,}") ~ "2+ fields", #3 fields
                str_detect(dupl, "[a-z|a-z\\_a-z]{1,}\\-[a-z|a-z\\_a-z]{1,}") ~ "2+ fields",
                TRUE ~ dupl)
          ) %>%
          # Make a new col to reorder the bars
          group_by(dupl) %>%
          mutate(total_count = n()) %>%
          ungroup() %>%
          ggplot(aes(x = reorder(dupl, -total_count))) +
          geom_bar(stat = "count") +
          geom_text(
            aes(
              x = dupl,
              label = stat(count),
              angle = 90),
            stat = "count",
            vjust = 0.2,
            hjust = -0.2) +
          labs(
            x = "Inconsistent field(s) for duplicated hunters",
            y = "Count",
            title = "Types of duplicates") +
          scale_y_continuous(expand = expansion(mult = c(-0, 0.2))) +
          theme_classic() +
          theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
        
      
    })
  },

  options = list(height = 500)
)
```

### After fixDuplicates (all 49 states)

```{r a_fd, echo = FALSE}
findDuplicates(fxd)
```

### After fixDuplicates (only permit states)

Permit states include AK, AZ, CO, KS, MN, MT, ND, NM, OK, OR, SD, TX, UT, WA, and WY.

```{r a_perms, echo = FALSE}
findDuplicates(fxd %>% filter(dl_state %in% perm_states))
```

## Strata

```{r stratacheck, echo = FALSE}
DT::datatable(strataCheck(fxd))
```

## Repeated values

### Horizontal

```{r h_validate, echo = FALSE}
DT::datatable(migbirdHarvestData::validate(fxd, type = "horizontal"))
```

### Vertical

```{r v_validate, echo = FALSE}
DT::datatable(migbirdHarvestData::validate(fxd, type = "vertical"))
```

```{r data_sweep, include = FALSE}

# Remove data objects to improve memory use
rm(cleaned_data)
rm(fxd)
rm(perm_states)
```

# Error Visualizations

## By field

### Before correction

```{r erpf, echo = FALSE}
errorPlot_fields(proofed_data, year = params$year)
```

### After correction

```{r erpf_c, echo = FALSE}
errorPlot_fields(corrected_data, year = params$year)
```

## By state

### Before correction

```{r erps, echo = FALSE}
errorPlot_states(proofed_data)
```

### After correction

```{r erps_c, echo = FALSE}
errorPlot_states(corrected_data)
```

## By download

### Before correction

```{r erpdl, echo = FALSE}
errorPlot_dl(proofed_data)
```

### After correction

```{r erpdl_c, echo = FALSE}
errorPlot_dl(corrected_data)
```

## Out-of-state hunters

```{r oosh, echo = FALSE}
outOfStateHunters(proofed_data)
```

## Youth hunters

```{r yh, echo = FALSE}
youthHunters(proofed_data, year = params$year)
```

# Error Exploration

Let's take a closer look at common errors and issues within this season's HIP data.

## High error proportions

Q: What states and fields had a proportion of error that exceeded an acceptable threshold (before correction)?

States with more than 1% error

```{r rfs, echo = FALSE}
redFlags(proofed_data, type = "state", threshold = 0.1)
```

Fields with more than 1% error

```{r rff, echo = FALSE}
redFlags(proofed_data, type = "field", threshold = 0.1)
```
